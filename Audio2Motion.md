# Audio2Motion
a reading list of audio2motion  
## Speech2Gesture
* [2025.03] [**MAG**: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization] [[PDF](https://arxiv.org/pdf/2503.14040)]
* [2025.03] [**ExGes**: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis] [[PDF](https://arxiv.org/pdf/2503.06499)]
* [2025.01] [**GestureLSM**: Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling] [[PDF](https://arxiv.org/pdf/2501.18898v1) [Page](https://andypinxinliu.github.io/GestureLSM)]
* [2024.10] [**SynTalker**: Enabling Synergistic Full-Body Control in Prompt-Based Co-Speech Motion Generation] [MM2024] [[PDF](https://arxiv.org/pdf/2410.00464v1)]
* [2024.03] [**MambaTalk**: Efficient Holistic Gesture Synthesis with Selective State Space Models] [NIPS2024][[PDF](https://arxiv.org/pdf/2403.09471v5)] 
* [2024.11] [**Dim-Gestor**: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2] [[PDF](https://arxiv.org/pdf/2411.16729)]
* [2024.08] [**MDT-A2G**: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation] [MM2024] [[Page](https://xiaofenmao.github.io/web-project/MDT-A2G/)]
* [2024.05] [**SIGGesture**: Generalized Co-Speech Gesture Synthesis via Semantic Injection with Large-Scale Pre-Training Diffusion Models] [SigGraph2024] [[PDF](https://arxiv.org/pdf/2405.13336)]
* [2024.05] [**Semantic Gesticulatar**: Semantics-Aware Co-Speech Gesture Synthesis] [SigGraph 2024] [[PDF](https://arxiv.org/pdf/2405.09814)]
* [2024.03] [**PG**: Speech-Driven Personalized Gesture Synthetics:Harnessing Automatic Fuzzy Feature Inference] [TVCG2024] [[PDF](https://arxiv.org/pdf/2403.10805)]
* [2024.03] [**ConvoFusion**: Multi-modal conversational diffusion for co-speech gesture synthesis] [CVPR2024] [[PDF](https://arxiv.org/pdf/2403.17936)]
* [2024.01] [**DiffSHEG**: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation] [CVPR2024] [[Page](https://jeremycjm.github.io/proj/DiffSHEG/)]
* [2024.01] [**EMAGE**: owards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling] [CVPR2024][[Page](https://pantomatrix.github.io/EMAGE/)]
* [2023.12] [**AMUSE**: Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion] [CVPR2024] [[PDF](https://arxiv.org/pdf/2312.04466)]
* [2023.08] [**DSG+**: The DiffuseStyleGesture+ entry to the GENEA Challenge 2023] [[PDF](https://arxiv.org/pdf/2308.13879)]
* [2023.05] [**QPGesture**: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation] [CVPR2023] [[PDF](https://arxiv.org/pdf/2305.11094)]
* [2023.05] [**DSG**: DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models] [[PDF](https://arxiv.org/pdf/2305.04919)]
* [2023.03] [**GestureDiffuCLIP**: Gesture Diffusion Model with CLIP Latents] [SigGraph2023] [[PDF](https://arxiv.org/pdf/2303.14613)]
* [2023.03] [**DiffGesture**: Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation] [CVPR2023] [[PDF](https://arxiv.org/pdf/2303.09119)]
* [2022.12] [**TalkShow**: Generating Holistic 3D Human Motion from Speech] [CVPR2023] [[Page](https://talkshow.is.tue.mpg.de/)]
* [2022.10] [**Ryhthmic Gesticulatar**: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings] [SigGraph 2022] [[PDF](https://arxiv.org/pdf/2210.01448)]
* [2022.03] [**BEAT**: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis] [ECCV2022] [[PDF](https://arxiv.org/pdf/2203.05297)]
* [2021.08] [**Audio2Gestures**: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders] [ICCV2021] [[PDF](https://arxiv.org/pdf/2108.06720)]
* [2022.07] [**Habibie et al**: A Motion Matching-based Framework for Controllable Gesture Synthesis from Speech] [SigGraph2022] [[PDF](https://vcai.mpi-inf.mpg.de/projects/SpeechGestureMatching/data/paper.pdf)]
* [2021.02] [**Habibie et al**: Learning Speech-driven 3D Conversational Gestures from Video] [IVA2021] [[PDF](https://arxiv.org/pdf/2102.06837)]
* [2019.06] [**Ginosar et al**: Learning Individual Styles of Conversational Gesture] [CVPR2019] [[PDF](https://arxiv.org/pdf/1906.04160)]
### Interaction Speech Motion
* [2024.12] [**It Takes Two**: Real-time Co-Speech Two-personâ€™s Interaction Generation via Reactive Auto-regressive Diffusion Model] [[PDF](https://arxiv.org/pdf/2412.02419)]
### Gesture Generation on Beat2
* [2025.01] [**GestureLSM**: Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling] [[PDF](https://arxiv.org/pdf/2501.18898v1) [Page](https://andypinxinliu.github.io/GestureLSM)]
* [2024.10] [**SynTalker**: Enabling Synergistic Full-Body Control in Prompt-Based Co-Speech Motion Generation] [MM2024] [[PDF](https://arxiv.org/pdf/2410.00464v1)]
* [2024.03] [**MambaTalk**: Efficient Holistic Gesture Synthesis with Selective State Space Models] [NIPS2024][[PDF](https://arxiv.org/pdf/2403.09471v5)]
* [2024.01] [**EMAGE**: owards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling] [CVPR2024][[Page](https://pantomatrix.github.io/EMAGE/)]

## Music2Dance
### On FineDance dataset
* [2025.02] [**GCDance**: Genre-Controlled 3D Full Body Dance Generation Driven By Music] [[PDF](https://arxiv.org/pdf/2502.18309)]
* [2024.10] [**lodge++**: High-quality and Long Dance Generation with Vivid Choreography Patterns] [[Page](https://li-ronghui.github.io/lodgepp)]
* [2024.03] [**lodge**: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives] [[Page](https://li-ronghui.github.io/lodge)]
* [2022.12] [**FineDance**: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation] [[PDF](https://arxiv.org/pdf/2212.03741)]
### On AIST++ dataset
* [2024.07] [**Beat It**: Beat-Synchronized Multi-Condition 3D Dance Generation] [ECCV2024] [[PDF](https://arxiv.org/pdf/2407.07554) [Page](https://zikaihuangscut.github.io/Beat-It/)]
* [2022.11] [**EDGE**: Editable Dance Generation from Music] [CVPR2023] [[Page](https://edge-dance.github.io/)]
* [2022.03] [**Bailando**: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory] [CVPR2022] [[PDF](https://arxiv.org/pdf/2203.13055) [Page](https://www.mmlab-ntu.com/project/bailando/index.html)]
* [2021.01] [**AIST++**: AI Choreographer: Music Conditioned 3D Dance Generation with AIST++] [ICCV2021] [[PDF](https://arxiv.org/pdf/2101.08779)]
### else
* [2025.02] [**X-Dancer**: Expressive Music to Human Dance Video Generation] [[PDF](https://arxiv.org/pdf/2502.17414)]
* [2024.09] [**DanceFusion**: A Spatio-Temporal Skeleton Diffusion Transformer for Audio-Driven Dance Motion Reconstruction] [[PDF](https://arxiv.org/pdf/2411.04646)]
### Interaction Dance
* [2024.12] [**InterDance**: REACTIVE 3D DANCE GENERATION WITH REALISTIC DUET INTERACTIONS] [[PDF](https://arxiv.org/pdf/2412.16982)]
* [2024.03] [**Duolando**: FOLLOWER GPT WITH OFF-POLICY REINFORCEMENT LEARNING FOR DANCE ACCOMPANIMENT] [ICLR2024] [[PDF](https://arxiv.org/pdf/2403.18811) [Page](https://lisiyao21.github.io/projects/Duolando/)]

