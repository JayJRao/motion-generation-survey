# Audio2Motion
a reading list of audio2motion  
## Speech2Gesture
* [2024.08] [**MDT-A2G**: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation] [MM2024] [[Page](https://xiaofenmao.github.io/web-project/MDT-A2G/)]
* [2024.05] [**SIGGesture**: Generalized Co-Speech Gesture Synthesis via Semantic Injection with Large-Scale Pre-Training Diffusion Models] [SigGraph2024] [[PDF](https://arxiv.org/pdf/2405.13336)]
* [2024.03] [**ConvoFusion**: Multi-modal conversational diffusion for co-speech gesture synthesis] [CVPR2024] [[PDF](https://arxiv.org/pdf/2403.17936)]
* [2024.01] [**DiffSHEG**: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation] [CVPR2024] [[Page](https://jeremycjm.github.io/proj/DiffSHEG/)]
* [2024.01] [**EMAGE**: owards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling] [CVPR2024][[Page](https://pantomatrix.github.io/EMAGE/)]
* [2023.08] [**DSG+**: The DiffuseStyleGesture+ entry to the GENEA Challenge 2023] [[PDF](https://arxiv.org/pdf/2308.13879)]
* [2023.05] [**QPGesture**: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation] [CVPR2023] [[PDF](https://arxiv.org/pdf/2305.11094)]
* [2023.05] [**DSG**: DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models] [[PDF](https://arxiv.org/pdf/2305.04919)]
* [2023.03] [**GestureDiffuCLIP**: Gesture Diffusion Model with CLIP Latents] [SigGraph2023] [[PDF](https://arxiv.org/pdf/2303.14613)]
* [2023.03] [**DiffGesture**: Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation] [CVPR2023] [[PDF](https://arxiv.org/pdf/2303.09119)]
* [2022.12] [**TalkShow**: Generating Holistic 3D Human Motion from Speech] [CVPR2023] [[Page](https://talkshow.is.tue.mpg.de/)]
* [2022.03] [**BEAT**: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis] [ECCV2022] [[PDF](https://arxiv.org/pdf/2203.05297)]
* [2021.08] [**Audio2Gestures**: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders] [ICCV2021] [[PDF](https://arxiv.org/pdf/2108.06720)]
* [2022.07] [**Habibie et al**: A Motion Matching-based Framework for Controllable Gesture Synthesis from Speech] [SigGraph2022] [[PDF](https://vcai.mpi-inf.mpg.de/projects/SpeechGestureMatching/data/paper.pdf)]
* [2021.02] [**Habibie et al**: Learning Speech-driven 3D Conversational Gestures from Video] [IVA2021] [[PDF](https://arxiv.org/pdf/2102.06837)]
* [2019.06] [**Ginosar et al**: Learning Individual Styles of Conversational Gesture] [CVPR2019] [[PDF](https://arxiv.org/pdf/1906.04160)]

## Music2Dance
### FineDance dataset
* [2024.10] [**lodge++**: High-quality and Long Dance Generation with Vivid Choreography Patterns] [[Page](https://li-ronghui.github.io/lodgepp)]
* [2024.03] [**lodge**: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives] [[Page](https://li-ronghui.github.io/lodge)]
* [2022.12] [**FineDance**: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation] [[PDF](https://arxiv.org/pdf/2212.03741)]
### AIST++ dataset
* [2024.07] [**Beat It**: Beat-Synchronized Multi-Condition 3D Dance Generation] [ECCV2024] [[PDF](https://arxiv.org/pdf/2407.07554) [Page](https://zikaihuangscut.github.io/Beat-It/)]
* [2022.11] [**EDGE**: Editable Dance Generation from Music] [CVPR2023] [[Page](https://edge-dance.github.io/)]
* [2022.03] [**Bailando**: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory] [CVPR2022] [[PDF](https://arxiv.org/pdf/2203.13055) [Page](https://www.mmlab-ntu.com/project/bailando/index.html)]
* [2021.01] [**AIST++**: AI Choreographer: Music Conditioned 3D Dance Generation with AIST++] [ICCV2021] [[PDF](https://arxiv.org/pdf/2101.08779)]

## New Dataset
* [2025.01] [**Motion-X++**: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset] [[PDF](https://arxiv.org/pdf/2501.05098)]
